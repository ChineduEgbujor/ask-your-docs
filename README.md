# Ask-Your-Docs

**Ask-Your-Docs** is a Retrieval-Augmented Generation (RAG) chatbot that lets you ask natural-language questions over your own document corpus. It combines:

- **Embeddings** (via Sentence-Transformers) to vectorize document chunks
- **FAISS** as a lightweight vector store for fast similarity search
- **FastAPI** to expose a `/query` endpoint that retrieves relevant passages
- **LLM integration** (Google Gemini or OpenAI) to generate concise, accurate answers
- **MLflow** for experiment tracking (embedding models, top-K, prompts)

---

## Features

1. **Document Ingestion**

   - Read plain-text files (`.txt`) from a folder
   - Compute embeddings with `sentence-transformers`
   - Build and persist a FAISS index + metadata mapping

2. **Semantic Retrieval**

   - Given a query, retrieve the top-K most semantically similar passages
   - Expose via `POST /query`

3. **RAG Answer Generation**

   - Combine retrieved passages into a prompt
   - Call an LLM (Gemini or OpenAI) to compose a final, context-aware answer
   - Return both the answer and the sources used

4. **Experiment Tracking**

   - Log parameters (model name, top-K, prompt template)
   - Log metrics (e.g. retrieval latency, basic user feedback scores)
   - Compare runs in the MLflow UI

5. **Optional Streamlit UI**
   - Simple web interface to type questions and see answers + sources
   - Toggle between embedding models or LLM backends

---

## 📂 Project Structure

```
ask-your-docs/
│
├── app/
│ ├── ingest.py # Ingest .txt docs → embeddings + FAISS index
│ ├── search.py # Load index + metadata, perform similarity search
│ └── main.py # FastAPI app exposing /query with RAG logic
│
├── data/
│ ├── docs/ # Place your .txt documents here
│ ├── faiss_index # FAISS index file
│ └── faiss_index.pkl # Pickled metadata (ids & texts)
│
├── ui.py # (Optional) Streamlit frontend for interactive Q&A
├── requirements.txt
├── .env # Environment variables (API keys, paths, URIs)
└── README.md # ← You are here
```

---

## ⚙️ Installation & Setup

1. **Clone the repo**

   ```bash
   git clone https://github.com/YOUR_USERNAME/ask-your-docs.git
   cd ask-your-docs
   ```

2. **Create & activate a Python virtual environment**

   ```bash
   python -m venv venv
   .\venv\Scripts\activate     # Windows
   ```

3. **Install dependencies**

   ```bash
   pip install --upgrade pip
   pip install -r requirements.txt
   ```

4. **Configure environment variables**  
   Copy .env.example to .env and fill in your values:
   ```ini
   OPENAI_API_KEY=your-openai-or-gemini-key
   VECTOR_STORE_PATH=./data/faiss_index
   MLFLOW_TRACKING_URI=http://localhost:5000  # or your MLflow server URL
   ```

## Phase 1: Ingest Documents

1. Place your .txt files in `data/docs/`

2. Run the ingestion script:
   ```bash
   python -m app.ingest --doc-dir data/docs
   ```
   This will build `data/faiss_index` and `data/faiss_index.pkl`.

## Phase 2: Run the API

1. Start FastAPI:

   ```bash
   uvicorn app.main:app --reload
   ```

2. Open the docs at http://localhost:8000/docs

3. Test the /query endpoint:
   ```bash
   curl -X POST http://localhost:8000/query \
     -H "Content-Type: application/json" \
     -d '{"question": "Your question here"}'
   ```

## Phase 3: Optional Streamlit UI

1. Launch the UI:

   ```bash
   streamlit run ui.py
   ```

2. Enter your question in the sidebar and see:
   - Answer generated by the LLM
   - Sources: the retrieved passages

## Phase 4: Experiment Tracking with MLflow

1. Start MLflow server (if not already running):

   ```bash
   mlflow ui --port 5000
   ```

2. As you call `/query`, runs will be logged.

3. Visit http://localhost:5000 to compare embedding models, top-K settings, prompt templates, and basic metrics.

## Next Steps & Extensions

- Custom Embedding Models: swap in all-MiniLM-L6-v2, paraphrase-MPNet, etc.
- Hybrid Retrieval: combine FAISS with a lightweight keyword search for edge cases.
- Advanced RAG: use LangChain for chain-of-thought, citation formatting, or multi-stage prompting.
- Authentication: add API key or OAuth before exposing your API publicly.
- Dockerization: containerize the service for consistent deployment.

## Contributing

Feel free to open issues or PRs! Whether it's adding new embedding models, improving prompts, or enhancing the UI, your contributions are welcome.

## License

This project is released under the MIT License.
